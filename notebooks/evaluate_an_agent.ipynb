{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to walk through 3 different evaluation strategies for LLM Systems. Basically, we'll evaluate three components of a system:\n",
    "\n",
    "1. `Final Response`: Evaluate the agent's final response.\n",
    "2. `Single step`: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).\n",
    "3. `Trajectory`: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following architecture for a SQL Agent:\n",
    "\n",
    "<img src=\"../images/sql-agent.png\" width=\"600\">\n",
    "\n",
    "This architecture suggests that we can evaluate the agent's final response, the agent's trajectory and the whether the expected tool call was invoked:\n",
    "\n",
    "<img src=\"../images/sql-agent-eval.png\" width=\"600\">\n",
    "\n",
    "So, based on that, let's build a SQL Agent and practice ways of evaluate that system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQLite database we are going to use here is the [f1db](https://github.com/f1db/f1db/) database, which is the most comprehensive free open source database with all-time Formula 1 racing data and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as Chinook.db\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Open a local file in binary write mode\n",
    "    with open(\"./database/Chinook.db\", \"wb\") as file:\n",
    "        # Write the content of the response (the file) to the local file\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded and saved as Chinook.db\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['Album',\n",
      " 'Artist',\n",
      " 'Customer',\n",
      " 'Employee',\n",
      " 'Genre',\n",
      " 'Invoice',\n",
      " 'InvoiceLine',\n",
      " 'MediaType',\n",
      " 'Playlist',\n",
      " 'PlaylistTrack',\n",
      " 'Track']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant√¥nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///database/Chinook.db\")\n",
    "\n",
    "print(db.dialect)\n",
    "pprint(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM Artist LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n",
    "experiment_prefix = \"sql-agent-gpt4o\"\n",
    "metadata = \"Chinook, gpt-4o-mini base-case-agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use SQL toolkit as well as some custom tools to check the query before executing it and check the query result from the database to confirm it is not empty or irrelevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QuerySQLDataBaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x106a00690>),\n",
       " InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x106a00690>),\n",
       " ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x106a00690>),\n",
       " QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x106a00690>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11010b590>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1112de750>, root_client=<openai.OpenAI object at 0x1109ec710>, root_async_client=<openai.AsyncOpenAI object at 0x11076ded0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11010b590>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1112de750>, root_client=<openai.OpenAI object at 0x1109ec710>, root_async_client=<openai.AsyncOpenAI object at 0x11076ded0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})),\n",
       " StructuredTool(name='check_query_tool', description='Use this tool to double check if your query is correct before executing it.', args_schema=<class 'langchain_core.utils.pydantic.check_query_tool'>, func=<function check_query_tool at 0x1132145e0>),\n",
       " StructuredTool(name='check_result', description='Use this tool to check the query result from the database to confirm it is not empty and is relevant', args_schema=<class 'langchain_core.utils.pydantic.check_result'>, func=<function check_result at 0x113291e40>)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "from langgraph.graph import MessageGraph\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "# SQL Toolkit\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "# Query checking\n",
    "query_check_prompt = \"\"\"\n",
    "You are a SQL expert with a strong attention to detail.\n",
    "Double check the SQLite query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "Execute the correct query with the appropriate tool.\"\"\"\n",
    "\n",
    "query_check_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=query_check_prompt),\n",
    "    HumanMessage(content=\"{query}\")\n",
    "])\n",
    "query_check = query_check_prompt | llm\n",
    "\n",
    "@tool\n",
    "def check_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to double check if your query is correct before executing it.\n",
    "    \"\"\"\n",
    "    return query_check.invoke({\"query\": query}).content\n",
    "\n",
    "\n",
    "query_result_check_prompt = \"\"\"\n",
    "You are grading the result of a SQL query from a DB.\n",
    "- Check that the result is not empty.\n",
    "- If it is empty, instruct the system to re-try!\n",
    "\"\"\"\n",
    "query_result_check_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=query_result_check_prompt),\n",
    "    HumanMessage(content=\"{query_result}\")\n",
    "])\n",
    "query_result_check = query_result_check_prompt | llm\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_result(query_result: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to check the query result from the database to confirm it is not empty and is relevant\n",
    "    \"\"\"\n",
    "    return query_result_check.invoke({\"query_result\": query_result}).content\n",
    "\n",
    "\n",
    "tools.extend([check_query_tool, check_result])\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "        \n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            # append to state\n",
    "            state = {**state}\n",
    "            \n",
    "            # invoke the tool-calling LLM\n",
    "            result = self.runnable.invoke(state)\n",
    "            \n",
    "            # if it is a tool call -> response valid\n",
    "            # if it has meaningful text -> response valid\n",
    "            # otherwise, we re-prompt it b/c response is not meaningful\n",
    "            no_tool_calls = not result.tool_calls\n",
    "            no_content = not result.content\n",
    "            content_is_list = isinstance(result.content, list)\n",
    "            no_text_in_first_element = content_is_list and not result.content[0].get(\"text\")\n",
    "\n",
    "            # Combine the conditions in a more explicit way\n",
    "            if no_tool_calls and (no_content or no_text_in_first_element):\n",
    "                messages = state[\"messages\"] + [HumanMessage(content=\"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import MessagesPlaceholder\n",
    "\n",
    "query_gen_prompt = \"\"\"\n",
    "ROLE:\n",
    "You are an agent designed to interact with a SQL database. You have access to tools for interacting with the database.\n",
    "GOAL:\n",
    "Given an input question, create a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "INSTRUCTIONS:\n",
    "- Only use the below tools for the following operations.\n",
    "- Only use the information returned by the below tools to construct your final answer.\n",
    "- To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step.\n",
    "- Then you should query the schema of the most relevant tables.\n",
    "- Write your query based upon the schema of the tables. You MUST double check your query before executing it.\n",
    "- Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "- You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "- Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "- If you get an error while executing a query, rewrite the query and try again.\n",
    "- If the query returns a result, use check_result tool to check the query result.\n",
    "- If the query result result is empty, think about the table schema, rewrite the query, and try again.\n",
    "- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=query_gen_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "assistant_runnable = query_gen_prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# nodes\n",
    "builder.add_node(\"assistant\", Assistant(runnable=assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
    "\n",
    "# edges\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Which country's customers spent the most? And how much did they spend?\",\n",
    "             \"How many albums does the artist Led Zeppelin have?\",\n",
    "             \"What was the most purchased track of 2017?\",\n",
    "             \"Which sales agent made the most in sales in 2009?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_id: 1e9129c7-3a3d-4a72-9d55-7266c327ad19\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Which country's customers spent the most? And how much did they spend?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_list_tables (call_jl3pn8EUm3lPuZVdJcV4sJDf)\n",
      " Call ID: call_jl3pn8EUm3lPuZVdJcV4sJDf\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_list_tables\n",
      "\n",
      "Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_schema (call_hgGG5LOvwsTw4QV7jYJhqnXC)\n",
      " Call ID: call_hgGG5LOvwsTw4QV7jYJhqnXC\n",
      "  Args:\n",
      "    table_names: Customer\n",
      "  sql_db_schema (call_VUn1qRzAulCekfskIRQHBYGh)\n",
      " Call ID: call_VUn1qRzAulCekfskIRQHBYGh\n",
      "  Args:\n",
      "    table_names: Invoice\n",
      "  sql_db_schema (call_rsTL6AzxQSndx6oij9PXycsB)\n",
      " Call ID: call_rsTL6AzxQSndx6oij9PXycsB\n",
      "  Args:\n",
      "    table_names: InvoiceLine\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_schema\n",
      "\n",
      "\n",
      "CREATE TABLE \"InvoiceLine\" (\n",
      "\t\"InvoiceLineId\" INTEGER NOT NULL, \n",
      "\t\"InvoiceId\" INTEGER NOT NULL, \n",
      "\t\"TrackId\" INTEGER NOT NULL, \n",
      "\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n",
      "\t\"Quantity\" INTEGER NOT NULL, \n",
      "\tPRIMARY KEY (\"InvoiceLineId\"), \n",
      "\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \n",
      "\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from InvoiceLine table:\n",
      "InvoiceLineId\tInvoiceId\tTrackId\tUnitPrice\tQuantity\n",
      "1\t1\t2\t0.99\t1\n",
      "2\t1\t4\t0.99\t1\n",
      "3\t2\t6\t0.99\t1\n",
      "*/\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_query_checker (call_UzmzgO4q7e74rIrXN4u3Xqzu)\n",
      " Call ID: call_UzmzgO4q7e74rIrXN4u3Xqzu\n",
      "  Args:\n",
      "    query: SELECT c.Country, SUM(i.Total) AS TotalSpent \n",
      "FROM Customer c \n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId \n",
      "GROUP BY c.Country \n",
      "ORDER BY TotalSpent DESC \n",
      "LIMIT 5;\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_query_checker\n",
      "\n",
      "```sql\n",
      "SELECT c.Country, SUM(i.Total) AS TotalSpent \n",
      "FROM Customer c \n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId \n",
      "GROUP BY c.Country \n",
      "ORDER BY TotalSpent DESC \n",
      "LIMIT 5;\n",
      "```\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  sql_db_query (call_UNcwefH322CB6UsMz8eiZrJX)\n",
      " Call ID: call_UNcwefH322CB6UsMz8eiZrJX\n",
      "  Args:\n",
      "    query: SELECT c.Country, SUM(i.Total) AS TotalSpent \n",
      "FROM Customer c \n",
      "JOIN Invoice i ON c.CustomerId = i.CustomerId \n",
      "GROUP BY c.Country \n",
      "ORDER BY TotalSpent DESC \n",
      "LIMIT 5;\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: sql_db_query\n",
      "\n",
      "[('USA', 523.06), ('Canada', 303.96), ('France', 195.1), ('Brazil', 190.1), ('Germany', 156.48)]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The country whose customers spent the most is the **USA**, with a total spending of **$523.06**.\n"
     ]
    }
   ],
   "source": [
    "# invoke\n",
    "import uuid\n",
    "\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "print(f\"thread_id: {thread_id}\")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "msg = {\"messages\": HumanMessage(content=questions[0])}\n",
    "for event in graph.stream(msg, config, stream_mode=\"values\"):\n",
    "    _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to evaluate how the agent system performs overall. Basically, we will treat the whole system as a black box and only care about the user input and final output.\n",
    "\n",
    "For that we need to create a dataset that evaluates end-to-end performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "examples = [\n",
    "    (\"Which country's customers spent the most? And how much did they spend?\", \n",
    "     \"The country whose customers spent the most is the USA, with a total expenditure of $523.06\"),\n",
    "    (\"What was the most purchased track of 2013?\", \n",
    "     \"The most purchased track of 2013 was Hot Girl.\"),\n",
    "    (\"How many albums does the artist Led Zeppelin have?\", \n",
    "     \"Led Zeppelin has 14 albums\"),\n",
    "    (\"What is the total price for the album ‚ÄúBig Ones‚Äù?\", \n",
    "     \"The total price for the album 'Big Ones' is 14.85\"),\n",
    "    (\"Which sales agent made the most in sales in 2009?\", \n",
    "     \"Steve Johnson made the most sales in 2009\"),\n",
    "]\n",
    "\n",
    "dataset_name = \"SQL Agent Response\"\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    inputs, outputs = zip(\n",
    "        *[\n",
    "            ({\"input\": text}, {\"output\": label}) for text, label in examples\n",
    "        ]\n",
    "    )\n",
    "    client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the chain\n",
    "def predict_sql_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    msg = {\"messages\": HumanMessage(content=example[\"input\"])}\n",
    "    # note that we are invoking graph\n",
    "    messages = graph.invoke(msg, config)\n",
    "    return {\"response\": messages[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a teacher grading a quiz. \n",
      "\n",
      "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
      "\n",
      "Here is the grade criteria to follow:\n",
      "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
      "(2) Ensure that the student answer does not contain any conflicting statements.\n",
      "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
      "\n",
      "Score:\n",
      "A score of 1 means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
      "A score of 0 means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
      "\n",
      "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
      "\n",
      "Avoid simply stating the correct answer at the outset.\n",
      "==================================================\n",
      "Input Variables:  ['correct_answer', 'question', 'student_answer']\n"
     ]
    }
   ],
   "source": [
    "# evaluator\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "print(grade_prompt_answer_accuracy.messages[0].prompt.template)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Input Variables: \", grade_prompt_answer_accuracy.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input\"]\n",
    "    reference = example.outputs[\"output\"]\n",
    "    prediction = run.outputs[\"response\"]\n",
    "    \n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n",
    "    \n",
    "    # structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "    \n",
    "    # run evaluator\n",
    "    score = answer_grader.invoke({\n",
    "        \"question\": input_question,\n",
    "        \"correct_answer\": reference,\n",
    "        \"student_answer\": prediction\n",
    "    })\n",
    "    score = score[\"Score\"]\n",
    "    \n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sql-agent-gpt4o-response-v-reference-2b1c58da' at:\n",
      "https://smith.langchain.com/o/7733aa42-7fa1-584c-b6b6-3775fbe407d6/datasets/58a182eb-911e-4749-8801-4e135c82df2b/compare?selectedSessions=3dc6ad4d-2689-4af7-a253-0f6e1dcd6976\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:14,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# create evaluation\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_sql_agent_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=experiment_prefix + \"-response-v-reference\",\n",
    "    num_repetitions=3,\n",
    "    metadata={\"version\": metadata}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of evaluating the overall system output, we are going to evaluate a single step, which basically involves evaluating the tool calls in our case.\n",
    "\n",
    "We can check a specific tool call using a [custom evaluator](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#use-custom-evaluators):\n",
    "\n",
    "- Here, we just invoke the assistant, assistant_runnable, with a prompt and check if the resulting tool call is as expected.\n",
    "- Here, we are using a specialized agent where the tools are hard-coded (rather than passed with the dataset input).\n",
    "- We specify the reference tool call for the step that we are evaluating, expected_tool_call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sql-agent-gpt4o-single-tool-ad1a6cf3' at:\n",
      "https://smith.langchain.com/o/7733aa42-7fa1-584c-b6b6-3775fbe407d6/datasets/58a182eb-911e-4749-8801-4e135c82df2b/compare?selectedSessions=3c8e6468-6130-4b84-8899-84a62055ac3d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:03,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def predict_assistant(example: dict):\n",
    "    \"\"\"Invoke assistant for a single tool call evaluation\"\"\"\n",
    "    msg = [HumanMessage(content=example[\"input\"])]\n",
    "    # note that we are invoking the assistant (prompt + llm)\n",
    "    result = assistant_runnable.invoke({\"messages\": msg})\n",
    "    return {\"response\": result}\n",
    "\n",
    "def check_specific_tool_call(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if the first tool call in the response matches the expected tool call.\n",
    "    \"\"\"\n",
    "    # expected tool call\n",
    "    expected_tool_call = \"sql_db_list_tables\"\n",
    "    \n",
    "    # run\n",
    "    response = root_run.outputs[\"response\"]\n",
    "    \n",
    "    # Get tool call\n",
    "    try:\n",
    "        tool_call = getattr(response, 'tool_calls', [])[0]['name']\n",
    "    except (IndexError, KeyError):\n",
    "        tool_call = None\n",
    "\n",
    "    score = 1 if tool_call == expected_tool_call else 0\n",
    "    return {\"score\": score, \"key\": \"single_tool_call\"}\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_assistant,\n",
    "    data=dataset_name,\n",
    "    evaluators=[check_specific_tool_call],\n",
    "    experiment_prefix=experiment_prefix + \"-single-tool\",\n",
    "    num_repetitions=3,\n",
    "    metadata={\"version\": metadata}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check a trajectory of tool calls using custom evaluators:\n",
    "\n",
    "- Here, we just invoke the agent, `graph.invoke`, with a prompt.\n",
    "- Here, we are using a specialized agent where the tools are hard-coded (rather than passed with the dataset input).\n",
    "- We extract the list of tools called, using `find_tool_calls`.\n",
    "- Custom functions can process these tool calls in various user-defined ways.\n",
    "- We can check if all expected tools are called in any order: `contains_all_tool_calls_any_order`\n",
    "- We can check if all expected tools are called in order, allowing for insertion of tool calls: `contains_all_tool_calls_in_order`\n",
    "- We can check if all expected tools are called in the exact order: `contains_all_tool_calls_in_order_exact_match`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'sql-agent-gpt4o-trajectory-3e5dd639' at:\n",
      "https://smith.langchain.com/o/7733aa42-7fa1-584c-b6b6-3775fbe407d6/datasets/58a182eb-911e-4749-8801-4e135c82df2b/compare?selectedSessions=365bd7b3-e04b-495f-8738-1b1bf5cff027\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:25,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "def predict_sql_agent_messages(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    msg = {\"messages\": HumanMessage(content=example[\"input\"])}\n",
    "    # note that we invoke the graph\n",
    "    messages = graph.invoke(msg, config)\n",
    "    return {\"response\": messages}\n",
    "\n",
    "\n",
    "def find_tool_calls(messages):\n",
    "    \"\"\"Find all tool calls in the messages returned\"\"\"\n",
    "    tool_calls = [tc[\"name\"] for m in messages[\"messages\"] for tc in getattr(m, 'tool_calls', [])]\n",
    "    return tool_calls\n",
    "\n",
    "\n",
    "def contains_all_tool_calls_any_order(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in any order.\n",
    "    \"\"\"\n",
    "    expected = ['sql_db_query_checker', 'sql_db_query', 'sql_db_schema', 'sql_db_list_tables']\n",
    "    messages = root_run.outputs[\"response\"]\n",
    "    tool_calls = find_tool_calls(messages)\n",
    "    # Optionally, log the tool calls -\n",
    "    # print(\"Here are my tool calls:\")\n",
    "    # print(tool_calls)\n",
    "    if set(expected) == set(tool_calls):\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return {\"score\": int(score), \"key\": \"multi_tool_call_any_order\"}\n",
    "\n",
    "\n",
    "def contains_all_tool_calls_in_order(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in exact order.\n",
    "    \"\"\"\n",
    "    messages = root_run.outputs[\"response\"]\n",
    "    tool_calls = find_tool_calls(messages)\n",
    "    # Optionally, log the tool calls -\n",
    "    # print(\"Here are my tool calls:\")\n",
    "    # print(tool_calls)\n",
    "    expected = ['sql_db_query_checker', 'sql_db_query', 'sql_db_schema', 'sql_db_list_tables']\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call in expected:\n",
    "            if len(expected) > 1:\n",
    "                expected = expected[1:]\n",
    "            else:\n",
    "                expected = []\n",
    "    \n",
    "    if len(expected) == 0:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return {\"score\": int(score), \"key\": \"multi_tool_call_in_order\"}\n",
    "\n",
    "\n",
    "def contains_all_tool_calls_in_order_exact_match(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Check if all expected tools are called in exact order and without any additional tool calls.\n",
    "    \"\"\"\n",
    "    expected = ['sql_db_list_tables',  'sql_db_schema', 'sql_db_query_checker', 'sql_db_query']\n",
    "    messages = root_run.outputs[\"response\"]\n",
    "    tool_calls = find_tool_calls(messages)\n",
    "    # Optionally, log the tool calls -\n",
    "    # print(\"Here are my tool calls:\")\n",
    "    # print(tool_calls)\n",
    "    tool_calls_order = []\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call not in tool_calls_order:\n",
    "            tool_calls_order.append(tool_call)\n",
    "            \n",
    "    if tool_calls_order == expected:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return {\"score\": int(score), \"key\": \"multi_tool_call_in_exact_order\"}\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_sql_agent_messages,\n",
    "    data=dataset_name,\n",
    "    evaluators=[contains_all_tool_calls_any_order, contains_all_tool_calls_in_order, contains_all_tool_calls_in_order_exact_match],\n",
    "    experiment_prefix=experiment_prefix + \"-trajectory\",\n",
    "    num_repetitions=3,\n",
    "    metadata={\"version\": metadata}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluating-llm-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
